{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression: Optimization of Long-Term Correction of Wind Data Using Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Description**\n",
    "\n",
    "In this assignment, you will work with realistic data from the wind industry. Vestas, a leading global company in wind energy, is interested in optimizing their methods for generating long-term corrected (LTC) wind data, which are used for planning the locations of new wind farms.\n",
    "\n",
    "**Context**\n",
    "\n",
    "Before building a new wind farm, Vestas needs to get an estimate of how much power it can produce. This can be done by running a simulation of the planned wind farm. The primary factors that decide how much power will be produced are: \n",
    "\n",
    "-  the turbine type (bigger turbines generally produce more energy), \n",
    "\n",
    "- the locations of the turbines (more energy is produced if the turbines are placed so that they do not block each other significantly) and \n",
    "\n",
    "- the wind speed and wind direction at the location where the wind farm is planned to be build. \n",
    "\n",
    "The turbine type and location is something that Vestas engineers get to decide, but wind is not so easily managed... Therefore, it is important to have a large amount of data to get a precise description of the distribution of wind speeds and wind directions at the potential site. Ideally a wind measuring mast would be build at the place of interest and collect data on wind speeds and wind directions for 20 years or so prior to the wind farm being build. However, the investers would probably become impatient if they had to wait for 20 years before construction could begin. \n",
    "\n",
    "So instead, a mast is build that collects data for a few years (typically 1-4 years). To account for more \"global\" variations in wind from year to year (some years are simply more windy than other years), the data from the mast is then compared to the data based on wind models which covers a much longer time scale. This model data (referred to as \"meso\" data) can be obtained for any location on Earth, and accounts for large scale wind variations (e.g. due to seasons, geography, Coriolis effect etc.). But it cannot be expected to give a precise description of the wind at a specific location, which is also affected by vegetation, buildings, the local landscape and so on. So in summary, the mast data captures the specific wind conditions for the given site, and the meso data accounts for variations in wind speeds on a longer time scale. Together, the two datasets usually give a good description of the wind at a specific site over a long time period, and therefore can be used to predict the expected power production over a long time span (e.g. 20 years, comparable to the life time of a wind farm).\n",
    "\n",
    "Below is an illustration comparing a meso and a mast time series. Note that this plot is only for illustration purposes. In reality, meso data typically has a frequency of 1 hour, and mast data has a frequency of 10 minutes. Besides, the figure only illustrates the variations in windspeeds. For the actual simulations, wind directions are also extremely important (e.g. to determine the locations of the turbines such that they don't block each other for the prevalent wind directions).\n",
    "\n",
    "![Illustration of meso vs mast time series](timeseries_example.png)\n",
    "\n",
    "In order to run a simulation for a given (potential) site, Vestas needs to obtain a single time series which has the same characteristics as the mast timeseries, but has a time span of 20 years like the meso time series. For this purpose, they currently use a neural network: They train the network on the overlapping parts of the mast and meso time series. Specifically, they train the network to be able to predict the mast wind speeds and wind directions based on the wind speeds and wind directions found in the meso data for the same time stamps. After the neural network has been trained, they feed the meso wind speeds and wind directions for the entire 20 years time span to get a *predicted* \"mast\" time series covering the 20 years of data found in the meso data. This *predicted* mast time series is called the \"long term corrected\" (LTC) time series, and is the one on which they base their simulations for the power production at the given site. \n",
    "\n",
    "However, traning neural networks is time consuming and expensive! Therefore, Vestas is curious if some kind of linear regression would be able to acheive comparable results. \n",
    "\n",
    "**Data**\n",
    " \n",
    "You will have access to two types of time series data:\n",
    "\n",
    "1) Mast time series data that represent the wind conditions at a specific location based on measurements from a wind measuring mast.\n",
    "\n",
    "2) Meso time series data that are based on weather measurement models and cover more than 20 years. While these data are less precise and don't exactly match the specific location, they provide a longer historical context.\n",
    "\n",
    "Note that the mast data for this project is significantly longer than \"typical\" mast data. This allows cutting the data into training and test sets (or training, validation and test sets). Each set should cover all four seasons.\n",
    "\n",
    "Your task is to develop a model using regression techniques that can generate LTC wind data. This LTC time series should be long, like the meso time series, but also give an accurate description of the wind conditions at the specific location.\n",
    "\n",
    "**Objectives and Purpose**\n",
    " \n",
    "The purpose of the assignment is to assess whether regression could be used instead of neural networks, which could potentially save time and money as it is generally quicker to perform a regression than to train a neural network. And the main objective is thus to develop a regression model that can generate LTC time series that are both accurate and cost-effective.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Please note. We have supplied some examples of how students have previously done the preprocessing to give you some input and thus make the work load a bit less. Feel free to use/reuse the preprocessing steps of these solutions.\n",
    "\n",
    "1. **Data preprocessing:** You must handle large datasets and perform necessary data preprocessing tasks. This includes dealing with missing values, handling outliers, and scaling data appropriately for the chosen regression technique. Feel free to use or get inspired by previous solutions.\n",
    "\n",
    "    a. Consider the appropriate intervals for wind speeds and wind directions. No negative wind speeds are allowed, and wind directions should be in an appropriate interval (e.g. [0; 360[ degrees).\n",
    "\n",
    "    b. Select which ws / wd signals to use. Signals at higher altitude are generally better, but it is even more important to have proper coverage of all seasons.\n",
    "\n",
    "    c. Find the meso-signals closest in height to the mast-data-signal you are using. Or interpolate the values between 2 or more meso-signals to get the values at the exact mast-signal-height.\n",
    "\n",
    "    d. Convert the mast data from DK time to UTC time (corresponding to the time zone used in the meso data). Remember to account for summer-time in DK.\n",
    "    \n",
    "    e. Resample the mast dataset to have the same frequency as the meso data. The meso data has one record for each hour, the mast data has one record for each 10 min.\n",
    "\n",
    "        i. Note: You should not convert the ws / wd signals to vector-quantities and use those for the resampling. Resample the ws and wd signal individually instead. The turbines “yaw” to always point toward the incoming wind, so the interesting value is the wind speed and not the wind velocity.  \n",
    "        ii. Be careful when resampling the wind directions. You don’t want the average of 0 degrees and 359 degrees to become ~180 degrees :-)\n",
    "\n",
    "    f. Find the overlapping timestamps between the meso data and the resampled mast data. You only want to consider data in this overlapping time period in your training.\n",
    "\n",
    "2. **Optional: Exploratory analysis:** You may do an exploratory analysis of the data, but this part is not mandatory. This includes presenting the data in tables and graphs, study and describe features of interest, as well as correlation analysis. Wind speeds typically follow a Weibull-distributions. Try fitting a Weibull distribution to the mast data, the resampled mast data and the meso-data. **(You can skip this part if you want to)**\n",
    "3. **Model Development:** Use appropriate machine learning principles and methodologies, including model training and testing and perhaps validation, cross-validation, and leave-one-out. You should apply and interpret regression models effectively for this task.\n",
    "4. **Model Evaluation:** Evaluate the developed model using appropriate metrics such as Mean Squared Error (MSE), R-squared (R²), etc. The evaluation should give an indication of the usefulness of your model in predicting wind conditions accurately. \n",
    "**Optional**: If you did step 2 above, in addition to calculating the above metric for your predicted time series, also consider comparing the distributions of the wind speeds in the predicted and the actual (resampled) mast data. That is, calculate the Weibull A- and k-parameters for both distributions, and find the error in these between your fit and the true data. \"Error-in-A\" and \"Error-in-k\" are the most used quantities to evaluate the long term correction process in the wind industry :-)\n",
    "5. **Documentation and Presentation:** Clearly document the steps, methodologies, and tools you used during the process. Present the results of your model in a clear and effective manner. This documentation should be comprehensive enough for someone to replicate your process and understand your results. Hand-in as one Jupyter Notebook.\n",
    "\n",
    "\n",
    "**Some additional comments**\n",
    "\n",
    "They (Vestas) have also tried running their own LTC algorithm on some of the data (they chose the 77m wind speed and wind direction signals from the Risø dataset), and this yielded good results. They assumed that the data was in Danish time, so they believe that is the case.\n",
    "\n",
    "As you can see, there is quite a focus on wind speeds (as they determine the amount of power produced). However, they know that their neural network operates by training on the different components of wind speed separately (meaning training one network on the x-component of the wind and another network on the y-component of the wind) and then combining them at the end. You may consider this method too. They are not sure if it yields better performance than training on wind speed and wind direction separately...but if time permits, give it a go.\n",
    "\n",
    "**About the data**\n",
    "\n",
    "The mast datasets are in netCDF format. It's quite easy to work with in Python (not sure if you've used it before?). In one of the folders, a test.py file is included that demonstrate how to load the dataset and access the most relevant mast signals.\n",
    "\n",
    "The mast data has a measurement frequency of 10 minutes, while the meso data has a frequency of 1 hour. Therefore, you will need to resample the mast data to a 1-hour frequency before you can use it (see requirement 1.e above). Vestas does the same with their data. As mentioned you should be careful when resampling the angles so that the average of 0 degrees (north) and 359 degrees doesn't end up being approximately 180 degrees.\n",
    "\n",
    "The data is publicly available. You can read more about it here:\n",
    "\n",
    "*Risø:*\n",
    "\n",
    "https://gitlab.windenergy.dtu.dk/fair-data/winddata-revamp/winddata-documentation/-/blob/kuhan-master-patch-91815/risoe_m.md\n",
    "\n",
    "Data: https://data.dtu.dk/articles/dataset/Wind_resource_data_from_the_tall_Ris_met_mast/14153204 (this is the \"DOI\"-link from the description)\n",
    "\n",
    "\n",
    "*Børglum:*\n",
    "\n",
    "https://gitlab.windenergy.dtu.dk/fair-data/winddata-revamp/winddata-documentation/-/blob/kuhan-master-patch-91815/borglum.md\n",
    "\n",
    "Data: https://data.dtu.dk/articles/dataset/Resource_data_from_the_Borglum_mast/14153231\n",
    "\n",
    "The two meso datasets come from Vestas' climate library, and the meso data is in UTC time. They don't think you need anything other than the \"wind speed\" (WSP) and \"wind direction\" (WDIR) signals. They believe it's most appropriate to either use the height closest to the mast height (for example, if you're using the wind speed signal ws125 and the wind direction signal wd125 from the Risø dataset, you should use WSP120 and WDIR120 from the meso dataset) or use multiple signals and interpolate to the desired height (125m) (see the requirements above).\n",
    "\n",
    "**Final comments**\n",
    "\n",
    "In a perfect world, you can do all of the above. The assignment is \"free\" in the sense that you should give the above a go and do your best. Remember, there is no right answer. This assignment is a real-world machine learning task and not a \"made-up\" school task. There are software engineers at Vestas working on exactly the same task (albeit with a different dataset, which they arent' allowed to share with us). But try to discuss the problem in your group and distribute the work among you. You can even collaborate with other groups or find inspiration in their approach.\n",
    "\n",
    "And remember. These portfolio assignments are not meant as \"learn stuff in class and apply to assignment\" - they are part of the learning process, and not simply a documentation of what you have learned. They should be seen as \"learning by doing\"-type assignments.\n",
    "\n",
    "In session 8, we will do a Q/A if you have any questions. But as mentioned, try to give it a go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data.zip from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "file_path_risoe = 'Data/Risoe/risoe_m_all.nc'\n",
    "file_paths_borglum = 'Data/Borglum/borglum_all.nc'\n",
    "\n",
    "signals_risoe = ['ws77', 'wd77', 'ws125', 'wd125']\n",
    "signals_borglum = ['ws32', 'wd32']\n",
    "\n",
    "# Define the base date for the datasets:\n",
    "base_date_borglum = datetime(1997, 12, 11, 16, 5, 0)\n",
    "base_date_risoe = datetime(1995, 11, 20, 16, 25, 0)\n",
    "\n",
    "# Get the Risoe dataset:\n",
    "dataset_risoe = nc.Dataset(file_path_risoe, 'r')\n",
    "# Get the Borglum dataset:\n",
    "dataset_borglum = nc.Dataset(file_paths_borglum, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Defininf usefull functions\n",
    "\n",
    "# List the variables in the dataset\n",
    "def print_dataset_info(dataset):\n",
    "    print(f\"{dataset.title}\")\n",
    "    print(\"Variables in the netCDF file:\")\n",
    "    for var_name in dataset.variables:\n",
    "        print(var_name)\n",
    "\n",
    "def convert_time(dataset, base_date):\n",
    "    time_minutes = np.array(dataset.variables['time'])\n",
    "\n",
    "    # Convert time values to timestamp strings\n",
    "    time = []\n",
    "    for minutes in time_minutes:\n",
    "        time_delta = timedelta(minutes=int(minutes))\n",
    "        timestamp = base_date + time_delta\n",
    "        time.append(timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    return time\n",
    "\n",
    "# Prining the first and last 10 values of the signals\n",
    "def print_signal_values(dataset, signals):\n",
    "\tfor signal in signals:\n",
    "\t\tvalues = np.array(dataset.variables[signal])\n",
    "\t\tprint(f'{signal}:\\n {values[:10]} - {values[-10:-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource data from the Risø met mast\n",
      "Variables in the netCDF file:\n",
      "time\n",
      "ws44\n",
      "ws44_qc\n",
      "ws77\n",
      "ws77_qc\n",
      "ws125\n",
      "ws125_qc\n",
      "wd77\n",
      "wd77_qc\n",
      "wd125\n",
      "wd125_qc\n",
      "t003\n",
      "t003_qc\n",
      "t044\n",
      "t044_qc\n",
      "t118\n",
      "t118_qc\n",
      "td01\n",
      "td01_qc\n",
      "rain\n",
      "rain_qc\n",
      "press\n",
      "press_qc\n",
      "rhum\n",
      "rhum_qc\n",
      "grad\n",
      "grad_qc\n"
     ]
    }
   ],
   "source": [
    "print_dataset_info(dataset_risoe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource data from Boerglum, Denmark\n",
      "Variables in the netCDF file:\n",
      "time\n",
      "ws32\n",
      "ws32_qc\n",
      "ws20\n",
      "ws20_qc\n",
      "ws10\n",
      "ws10_qc\n",
      "wd10\n",
      "wd10_qc\n",
      "wd32\n",
      "wd32_qc\n",
      "t002\n",
      "t002_qc\n",
      "t30\n",
      "t30_qc\n",
      "td10_2\n",
      "td10_2_qc\n",
      "td30_10\n",
      "td30_10_qc\n",
      "rhum\n",
      "rhum_qc\n",
      "grad\n",
      "grad_qc\n",
      "press\n",
      "press_qc\n"
     ]
    }
   ],
   "source": [
    "print_dataset_info(dataset_borglum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6334/245971336.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  time_minutes = np.array(dataset.variables['time'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\n",
      " ['1995-11-20 16:25:00', '1995-11-20 16:35:00', '1995-11-20 16:45:00', '1995-11-20 16:55:00', '1995-11-20 17:05:00', '1995-11-20 17:15:00', '1995-11-20 17:25:00', '1995-11-20 17:35:00', '1995-11-20 17:45:00', '1995-11-20 17:55:00'] - 2007-12-31 23:56:00\n",
      "time:\n",
      " ['1997-12-11 16:05:00', '1997-12-11 16:15:00', '1997-12-11 16:25:00', '1997-12-11 16:35:00', '1997-12-11 16:45:00', '1997-12-11 16:55:00', '1997-12-11 17:05:00', '1997-12-11 17:15:00', '1997-12-11 17:25:00', '1997-12-11 17:35:00'] - 2001-12-31 23:55:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert time for the Risoe dataset\n",
    "time_risoe = convert_time(dataset_risoe, base_date_risoe)\n",
    "print(f\"time:\\n {time_risoe[:10]} - {time_risoe[-1]}\")\n",
    "\n",
    "# Convert time for the Borglum dataset\n",
    "time_borglum = convert_time(dataset_borglum, base_date_borglum)\n",
    "print(f\"time:\\n {time_borglum[:10]} - {time_borglum[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws77:\n",
      " [3.36 3.05 3.59 3.87 4.74 4.91 4.98 5.39 5.76 5.52] - [8.14 8.71 6.82 7.26 7.24 6.04 6.97 8.17 6.66]\n",
      "wd77:\n",
      " [205. 205. 204. 202. 201. 206. 203. 203. 193. 200.] - [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "ws125:\n",
      " [3.04 3.17 3.64 3.77 4.28 4.91 5.35 5.58 5.75 5.38] - [nan nan nan nan nan nan nan nan nan]\n",
      "wd125:\n",
      " [208. 214. 209. 209. 212. 213. 210. 206. 207. 205.] - [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "ws32:\n",
      " [6.95 7.19 7.22 7.69 7.35 6.75 6.62 5.98 5.78 5.95] - [1.57 1.24 1.73 1.87 1.9  1.73 1.83 2.   2.13]\n",
      "wd32:\n",
      " [nan nan nan nan nan nan nan nan nan nan] - [ 80. 111. 140. 152. 174. 173. 160. 163. 173.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6334/245971336.py:25: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  values = np.array(dataset.variables[signal])\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "print_signal_values(dataset_risoe, signals_risoe)\n",
    "print_signal_values(dataset_borglum, signals_borglum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # 1. Find the maximum start time and minimum end time for overlap\\nstart_time = max(time_hourly[0], time_10min[0])\\nend_time = min(time_hourly[-1], time_10min[-1])\\n\\n# 2. Truncate both arrays based on the overlapping range\\ntime_hourly_truncated = time_hourly[(time_hourly >= start_time) & (time_hourly <= end_time)]\\ntime_10min_truncated = time_10min[(time_10min >= start_time) & (time_10min <= end_time)]\\n\\n# Print results\\nprint(\"Truncated hourly timestamps:\")\\nprint(time_hourly_truncated)\\n\\nprint(\"\\nTruncated 10-minute timestamps:\")\\nprint(time_10min_truncated) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # 1. Find the maximum start time and minimum end time for overlap\n",
    "start_time = max(time_hourly[0], time_10min[0])\n",
    "end_time = min(time_hourly[-1], time_10min[-1])\n",
    "\n",
    "# 2. Truncate both arrays based on the overlapping range\n",
    "time_hourly_truncated = time_hourly[(time_hourly >= start_time) & (time_hourly <= end_time)]\n",
    "time_10min_truncated = time_10min[(time_10min >= start_time) & (time_10min <= end_time)]\n",
    "\n",
    "# Print results\n",
    "print(\"Truncated hourly timestamps:\")\n",
    "print(time_hourly_truncated)\n",
    "\n",
    "print(\"\\nTruncated 10-minute timestamps:\")\n",
    "print(time_10min_truncated) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997-12-11 16:05:00 2001-12-31 23:55:00\n"
     ]
    }
   ],
   "source": [
    "start_time = max(time_risoe[0], time_borglum[0])\n",
    "end_time = min(time_risoe[-1], time_borglum[-1])\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1997, 1998, 1999, 2000, 2001]\n",
      "[1997, 1998, 1999, 2000, 2001]\n"
     ]
    }
   ],
   "source": [
    "years_borglum = [datetime.strptime(t, '%Y-%m-%d %H:%M:%S').year for t in time_borglum]\n",
    "years_borglum_sorted = sorted(years_borglum)\n",
    "years_borglum_sorted = list(sorted(set(years_borglum)))\n",
    "print(years_borglum_sorted[:10])\n",
    "print(years_borglum_sorted[-10:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mast Start: 1995-11-20 16:25:00\n",
      "Mast End: 2007-12-31 23:56:00\n",
      "Meso Start: 2000-01-01 07:00:00\n",
      "Meso End: 2023-06-11 06:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "\n",
    "file_path_risoe_mast = 'Data/Risoe/risoe_m_all.nc'\n",
    "file_paths_risoe_meso = 'Data/Risoe/meso_Risoe.csv'\n",
    "\n",
    "mast_df = xr.open_dataset(file_path_risoe_mast).to_dataframe().reset_index()\n",
    "meso_df = pd.read_csv(file_paths_risoe_meso, parse_dates=['TIMESTAMP'])\n",
    "\n",
    "mast_df = mast_df[['time', 'ws77', 'wd77']]\n",
    "meso_df = meso_df[['TIMESTAMP', 'WSP080', 'WDIR080']]\n",
    "\n",
    "#time frame of meso data\n",
    "print('Mast Start:', mast_df['time'].min())\n",
    "print('Mast End:', mast_df['time'].max())\n",
    "\n",
    "#time frame of mast data\n",
    "print('Meso Start:', meso_df['TIMESTAMP'].min())\n",
    "print('Meso End:', meso_df['TIMESTAMP'].max())\n",
    "\n",
    "# The meso and mast data overplap from 2000 to 2007\n",
    "\n",
    "\n",
    "# mast_df.set_index('time', inplace=True)\n",
    "# meso_df.set_index('TIMESTAMP', inplace=True)\n",
    "\n",
    "# Filter the time to be from 2008 to 2012\n",
    "# mast_df = mast_df[(mast_df.index >= '2008-01-01') & (mast_df.index <= '2012-12-31')]\n",
    "# meso_df = meso_df[(meso_df.index >= '2008-01-01') & (meso_df.index <= '2012-12-31')]\n",
    "\n",
    "\n",
    "\n",
    "# mast_df.plot(subplots=True, layout=(4,1), figsize=(12,12), sharex=False, sharey=False)\n",
    "# meso_df.plot(subplots=True, layout=(4,1), figsize=(12,12), sharex=False, sharey=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting the time frame of the meso and mast from 2000 - 2007\n",
    "meso_df = meso_df[(meso_df['TIMESTAMP'] >= '2000-01-01 07:00:00') & (meso_df['TIMESTAMP'] <= '2007-12-31 23:00:00')]\n",
    "mast_df = mast_df[(mast_df['time'] >= '2000-01-01 07:00:00') & (mast_df['time'] <= '2007-12-31 23:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      time            time_utc\n",
      "216373 2000-01-01 07:05:00 2000-01-01 06:05:00\n",
      "216374 2000-01-01 07:15:00 2000-01-01 06:15:00\n",
      "216375 2000-01-01 07:25:00 2000-01-01 06:25:00\n",
      "216376 2000-01-01 07:35:00 2000-01-01 06:35:00\n",
      "216377 2000-01-01 07:45:00 2000-01-01 06:45:00\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "def convert_to_utc(df, time_column):\n",
    "    def is_dst(date):\n",
    "        # Last Sunday in March\n",
    "        last_sunday_march = max(week[-1] for week in calendar.monthcalendar(date.year, 3))\n",
    "        # Last Sunday in October\n",
    "        last_sunday_october = max(week[-1] for week in calendar.monthcalendar(date.year, 10))\n",
    "        return date >= datetime(date.year, 3, last_sunday_march) and date < datetime(date.year, 10, last_sunday_october)\n",
    "\n",
    "    df['time_utc'] = df[time_column].apply(lambda x: x - timedelta(hours=2) if is_dst(x) else x - timedelta(hours=1))\n",
    "    return df\n",
    "\n",
    "# Convert the mast_df time to UTC\n",
    "mast_df = convert_to_utc(mast_df, 'time')\n",
    "print(mast_df[['time', 'time_utc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['time_utc'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6334/611872522.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set the 'time_utc' column as the index for resampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmast_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_utc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Resample the data to 1-hour intervals using mean for wind speed and circular mean for wind direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m resampled_mast_df = mast_df.resample('H').agg({\n",
      "\u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6118\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mNone of \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m are in the columns\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['time_utc'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Set the 'time_utc' column as the index for resampling\n",
    "mast_df.set_index('time_utc', inplace=True)\n",
    "\n",
    "# Resample the data to 1-hour intervals using mean for wind speed and circular mean for wind direction\n",
    "resampled_mast_df = mast_df.resample('H').agg({\n",
    "    'ws77': 'mean',\n",
    "    'wd77': lambda x: np.arctan2(np.mean(np.sin(np.deg2rad(x))), np.mean(np.cos(np.deg2rad(x)))) * (180 / np.pi)\n",
    "})\n",
    "\n",
    "# Reset the index to make 'time_utc' a column again\n",
    "resampled_mast_df.reset_index(inplace=True)\n",
    "\n",
    "# Print the resampled data\n",
    "print(resampled_mast_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
